\documentclass[12pt]{scrreprt}
\usepackage{listings}
\usepackage{underscore}
\usepackage{minted}
\usepackage[bookmarks=true]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{tcolorbox}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}


\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays

\newcommand{\pr}{\mathbb{P}}

\hypersetup{
    bookmarks=false,    % show bookmarks bar?
    pdftitle={FP PROJECT REPORT},    % title
    pdfauthor={Saptarishi Dhanuka},                     % author
    pdfsubject={TeX and LaTeX},                        % subject of the document
    pdfkeywords={TeX, LaTeX, graphics, images}, % list of keywords
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=blue,       % color of internal links
    citecolor=black,       % color of links to bibliography
    filecolor=black,        % color of file links
    urlcolor=purple,        % color of external links
    linktoc=page            % only page is linked
}%
\def\myversion{1.0 }
\date{}
%\title{%

%}
\usepackage{hyperref}
\newcommand{\ttt}[1]{\texttt{#1}}


\begin{document}




% ·        Title (with name, ID, supervisor, date)

% ·        Acknowledgement, if any

% ·        Table of Contents

% ·        Introduction

% ·        Background and Motivation

% ·        Literature Survey (state-of-the-art) – Gap Analysis, Research Questions etc.

% ·        Problem Statement / Objectives

% ·        Scope, Methodology, and Design – Architecture, HLD / LLD, if applicable

%  can have naive bayes in the methodology

% ·        Work Done – Implementations, Challenges, Mitigations, etc.

% ·        Results and Discussions

% ·        Conclusions (should highlight contribution)

% ·        Extensions and Future Work

% ·        References

 

% NOTES
% midterm feedback: First the specs should just contain the input output and not the method
% Req
% Spec
% Analysis
% Then architecture and design

% Explore llm for identifying interesting

% Explore pldi type thing

% First the project assumes html on the basis of tags. If not based on tags the
% the thing breaks. The specs shouldnt mention tagsoup and stuff. Should be independent of the method. But it should mention the limitations (?)

% In specs reduce the scope to assuming that using html and the tags are there using the parenthesisation problem. No don't do that in specs do that in the design  

% Original requirement doesn't mention that tags are there

% Just web page and get code and text

% Can u identify the code and the text without knowing the tags

% Mention in analysis

% Design shouldnt have the function details and stuff

% Architecture and design comes after analysing the the specs and requirements

% Specs in the presentation are wrong

% Get feedback from prof about what to do in report



% The specs are wrong. The specs should firstly mention the input output again after the requirments and mention it more technically like what separates the code from the text. It should NOT mention tagsoup or anything related to the method


% In the analysis of the specs then we can say that one way to tackle this problem is by assuming that the given web page is formatted in an correct HTML structure. Then we can extract the tags and solve the parenthesization problem for getting the code and the text out





% design flows from the specs so shouldn't have the function details and stuff directly 


\begin{flushright}
    \rule{16cm}{5pt}\vskip1cm
    \begin{bfseries}
        \Huge{Functional Programming \\ CS-IS-2010-1 \\ Final Project Report }\\
        \vspace{1.1cm}
        Topic: Haskell Scraper and Code-Text Separation\\
        \vspace{1.1cm}
        Name: Saptarishi Dhanuka\\
        \vspace{1.1cm}
        ID: 1020211525\\
        \vspace{1.1cm}
        Supervisor: Partha Pratim Das\\
        \vspace{1.1cm}
        Date: 13 May 2024\\
        \vspace{1.1cm}
        Ashoka University\\
    \end{bfseries}
\end{flushright}

\tableofcontents


\chapter{Acknowledgements}

I would like to thank Professor Das for taking this Independent Study Module and giving us the incentive to work on a software project in an unfamilar yet elegant language. I would also like to thank my fellow students as well as the teaching volunteers Gautam and Adwaiya for creating a wholesome learning environment. 


\chapter{Introduction}

%  cite stuff here cause references page is there and can't just say stuff in the air
%  maybe some motivation part here can go into the motivation section instead of the intro 

The large amounts of data in the form of interspersed code and natural language make it useful to extract knowledge from them in order to benefit various areas of software development. Examples like natural language comments accompanying code, stackoverflow questions and answers with code blocks, and developer emails containing discussions about code with reference to it are all important forms of such data. \\ 
\\ INSERT STUFF HERE!!!!!!!\\   
\\ For instance, extracting code and natural language separately from developer emails at a company can give key insights about how the company codebase has evolved and can help future developers access the code of emails directly. Separation of code and natural language is also helpful as training data for natural language and code completion models which can used the separated text and code to give better results. It creates a structured view of the code and textual data which can then further be used to create an organized view of the code and associated natural language. 


\chapter{Background and Motivation}


Consider the following image of an educational web page containing interspersed code and natural language:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/background-eg.png}
    \caption{Example of Code and Language Together}
    \label{fig:high-level-arch}
\end{figure}

Here it becomes useful to extract just the code or just the natural language separately if one just wants to run the code on their own machine or just wants the natural language descriptions for their notes or to get an overview of the idea being conveyed. Using HTML based separation of the text and code snippets, or trying to use regular expressions severely restricts the generalisability and accuracy of the system in extracting natural language and code separately.\\
\\ INSERT STUFF HERE!!!!!!!\\
\\ Towards this purpose, we will create a Naive Bayes classifier for classifying a particular line as natural language or code and creating two separate sections for all the natural language and all the code

\chapter{Literature Survey}

%  maybe also see what all libraries and stuff for naive bayes and stuff are there in haskell rn

%  gap analysis and research questions ?????????????????

% basically that these guys did this and those guys did that and here's what I am gonna do and mention shortcomings and stuff later and say that did from scratch since haskell sde project
% should i mention the results of the literature here or when comparing with my own results in the results section??



Researchers have used various methods to identify and distinguish code and natural language text, some of which I will discuss below. \\
\\ Bacchelli et. al \cite{bachemail} attempted to extract source code from developer emails by first identifying emails that contained source code and then identifying code blocks by using regular expressions. The approach was very lightweight and programming language specific, with the use of end-of-line characters like semi-colons in order to find source code lines. \\ Bacchelli et. al \cite{bachcontent} also used Naive Bayes with bigrams, in combination with island parsing for classifying pieces of development emails as various categories like code, natural language, stack trace, patch or junk. It reached an accuracy ranging between 89 and 94 \% but, the source code island parser was specific to Java, which the authors acknowledged and stated that the approach could be easily generalized to other language as well. \\ Chatterjee et. al \cite{chattter} focused on extracting the code segments along with their natural language descriptions from research articles using various detailed linguistic and structural heuristics to identify important features to identifying sentences that would be relevant to code segments \\ Cerulo et. al \cite{cerulohmm} used an approach based on Hidden Markov Models (HMMs) for extracting "information islands" of code from natural language by recognising whether the sequence of observed strings would change between the source code and natural language states of the HMM. While this approach did not require parsing or complicated regular expressions, it only performed classification of tokens into code or text, and not further knowledge extraction in the form of an abstract syntax tree.\\
\\ The Naive Bayes approach for text classification also has a long history and it is extensively used even today despite it's simplicity and strong assumptions. \\
\\ ADD STUFF HERE!!!!!!!!!!!!!






\chapter{Problem Statement and Objectives}
\begin{tcolorbox}[colback=white,colframe=gray,title={Assigned Project Statement}]
    Develop a scraper using Haskell to extract text and code snippets separately.
    \begin{enumerate}
        \item \textbf{Input}: Scrape the text and code snippets from the given text \href{https://eli.thegreenplace.net/2018/type-erasure-and-reification/}{source}
        \item \textbf{Output}: A Word document containing the text and \texttt{.txt} file containing the code.
        \item \textbf{Method}: Write the algorithm to scrape (you can use the \texttt{tagsoup} library) and all the input-output facilities using Haskell. Do not use any other language.
    \end{enumerate}
\end{tcolorbox}

% in the design section can give the basic structure of the HTML page

% \section{Problem Description}
% The given web page is made of text and code snippets, which we need to scrape and extract separately into a \texttt{.docx} file containing the text portions and a \texttt{.txt} file which has the code snippets. \\ For this, we need to fetch the given web page, parse and analyze it,so that we can effectively separate them into different documents.


\section{Requirements/Objectives}

\begin{enumerate}
    \item The user shall be able to give any text source as input.
    \item The scraper shall get all the code snippets of the source and write it into a Plaintext file.
    \item The scraper shall get all non-code text of the source and write it into a Word Document.
\end{enumerate}


\section{Specifications}

\begin{enumerate}
    \item The user will be able to enter a text source as input, whose code and non-code parts they wish to be separated.
    \item The scraper will parse the contents of the text and separate the code snippets from the rest of the text.
    \item The scraper will output a \texttt{.docx} file containing the textual content.
    \item The scraper will output a \ttt{.txt} file containing the code snippets.  
\end{enumerate}



% can mention that the scope is being restricted to with HTML stuff in the analysis
\section{Analysis}

\begin{enumerate}
    \item There are many ways of solving the problem both by syntactic and semantic approaches. Some semantic approaches are as follows:
        \begin{enumerate}
            \item Lexical and semantic analysis with the use of regular expressions
            \item Using a large language model to differentiate the code and the rest of the text 
            \item Using computer vision to attempt to read text like a human and identify text from the code
            \item Use the frequency of occurrence of different words in some sample data and use it to predict whether sections of unseen samples of text are natural language or source code. 
        \end{enumerate}
    \item Extracting the text and code snippets from a text source boils down to a classification task where we consider each new line as a line to be classified as natural language or code, and grouping them all into two separate sections depending on the class assigned to them.
    \item Hence we consider that the text source will consist of different newlines which need to be classified as language or code, without going into further granular details as to whether a particular word or phrase is code or text.
    %  we will take as input url but then just get the text from that url and nothing else. 
    

\end{enumerate}






\chapter{Scope and Methodology}

\section{Scope}
Following from the analysis, the system will classify text sources on a line level of granularity, hence it assumes that there will be some newline separation of different lines and the text source will not be completely unstructured. This is still a fairly broad scope since most text sources that have natural language and code interspersed in them have a newline-structure that make it easy to split them on a line-by-line basis for classification. 

% ADD STUFF HEREEEEEE!!!!

\section{Methodology}

The broad methodology is to train a Naive Bayes classifier on some custom training data consisting of natural language and source code, and then using the calculated probabilities to classify each line in the given text source as being code or natural language, and then separating them based on that. A brief overview of Naive Bayes is given below.
\subsection{Naive Bayes}
Our main goal with Naive Bayes is to find, for each line in the text source, the best class (code or natural language) for it. That is, for each line $l$ made of some $t_k$ terms and class $c$, we want to find \\ $\hat{c}$ = $ \argmax_c \pr(c | l) = \argmax_c \pr(c | t_1, t_2, ... t_k)$. Instead of just considering the terms occuring in the line, we will consider all words in the vocabulary from now and see if they are occuring in the line or not.\\
\\ First we consider the vocabulary of all the words in the training data. Now for each line in the text source, we have a binary feature vector whose elements are set to 1 if a word in the vocabulary occurs in that line, otherwise being set to 0. That is, if there are $n$ words in the vocabulary, then the feature vector $l_i$ corresponding to the $i$-th line will have $l_{ij} = 1$ if the $j$-th word from vocabulary occurs in the $i$-th line, otherwise being 0.\\
\\ Now we want to calculate $\mathbb{P}(l_i | y)$ where $y$ is a particular class like code or natural language. We make the strong Naive Bayes assumption that all the $l_{ij}$'s are conditionally independent given the class $y$. This is clearly not true in practice, but even with this assumption we get good performance. 
Moreover, we consider each line as an unordered Bag of Words where word position doesn't matter and only it's frequency matters, which is another strong assumption that doesn't impact performance that much.\\
\\ Hence $\mathbb{P}(l_{i1}...l_{in} | y)$ = $\prod_{j=1}^{n} \mathbb{P}(l_{ij} | y)$. \\
\\ Considering a particular line $l$ and using Bayes Rule we get:\\
\\  $\hat{c}$ = $ \argmax_c \pr(c | l) = \argmax_c \pr(l | c) \times \pr(c)  = \argmax_c  \pr(c) \times \prod_{j=1}^{n} \mathbb{P}(l_{j} | c) $.\\
\\ Usually, the probability numbers computed are very small, so we work in the logspace by using the natural logarithm to avoid issues with numerical underflow.\\

$\therefore \hat{c} =  \argmax_c \log \pr(c | l) = \argmax_c \log \pr(l | c) \times \pr(c) = \argmax_c  \log \pr(c) \times \prod_{j=1}^{n} \mathbb{P}(l_{j} | c)  \\
\\ = \argmax_c (\log \pr(c) + \sum_{j=1}^{n} \pr(l_{j} | c)) $\\
\\ \textbf{How to calculate these terms?}\\
\\ We calculate $\pr(c)$ by just calculating what percentage of the training data belongs to that particular class. For each $\pr(l_j|c)$, we calculate the probability of the $j$-th word in the vocabulary being in the class $c$ by counting the number of times that word occurs in the class $c$ and dividing by the total number of words in the class $c$. Using a form of Laplace smoothing, we add small decimal numbers $\alpha$ and $\beta$ in the numerator and denominator to avoid 0 probability in case of completely unseen words for the unseen data.\\
\\ Hence $\pr(l_j|c) = \dfrac{count(l_j, c) + \alpha}{\beta + \sum_{j=1}^{n} count(l_j, c)}$







\chapter{Design and Architecture}
% need to put functions into Lib and main stuff in main
% mention directory structure here along with basic ideas of the functions and code what it's doing


% ·        Scope, Methodology, and Design – Architecture, HLD / LLD, if applicable


%  can have the design of the pipeline system from the text source to the two text files here. can do the results and stuff with the tables and the graphs later

\section{High-Level Architecture}

% insert image here
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/NB-high-arch.png}
    \caption{High-Level Architecture of Code-Text Separation Pipeline}
    \label{fig:high-level-arch}
\end{figure}


% change this cause it is not consistent with the design later need to make it so that the actual things like the tags and the text tags are in the boxes and the processes are on the arrows. 
The \textbf{high-level architecture} consists of the following:
\begin{enumerate}
    \item Get the contents of the text source
    % \item Parsing the response obtained from the HTTP libraries into Tags from the \texttt{tagsoup} library.
    % \item Separating the text from the code snippets using the descriptions of each Tag from the above Tags
    \item Separate the code snippets from the text.
    \item Writing the code snippets into a \texttt{.txt} file.
    \item Write the non-code textual content into a \texttt{.docx} file.
\end{enumerate}


\newpage 


\section{High-Level Design}


\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/NB-high-design.png}
    \caption{High-Level Design of the Classifying Pipeline and Training Section}
    \label{fig:high-level-design}
\end{figure}

A more detailed description of the \textbf{high-level design} which implements the architecture broadly consists of two sections:
\begin{enumerate}
    \item \textbf{Training Section : }
    \begin{enumerate}
        \item Use natural language and source code training data already pre-classified in order to calculate the conditional probabilities and the class priors which make up the trained Naive Bayes Model.
    \end{enumerate}
    



    \item \textbf{Separation Pipeline Section : }
    \begin{enumerate}
        \item Split the given text source into lines which will be considered as separate "documents" by the trained classificaton model.
        \item Classify each different line as source code or natural language text depending on which class has the higher posterior probability given that line
        \item Write the lines classified as source code into a \texttt{.txt} file
        \item Write the lines classified as natural language text into a \texttt{.docx} file
        % \item Execute an HTTPS request for fetching the HTML contents of the web page
        % % \item Parsing the url into a request
        % % \item Executing the request with the TLS manager
        % \item Get the body i.e. the HTML content from the response received after executing the request
        % \item Parse the HTML into a list of Tags according to the \texttt{tagsoup} library
        % \item Separate the Tags corresponding to the code from the Tags corresponding to the textual content. By inspecting the HTML, we can see that the code snippets are within \texttt{<pre>} tags, so we need to separate everything enclosed within these tags from the rest of the HTML content. We also remove images. 
        % \item Insert delimiters between each different code snippet for formatting purposes. 
        % % \item Convert the list of Tag Strings corresponding to the code and to the text each back into an HTML-formatted string, which we then convert into a \texttt{pandoc} document as intermediate representation
        % % \item Convert the pandocs into another intermediate string-like format which can then be written into the respective \texttt{.docx} and \texttt{.txt} files
        % \item Convert the Tags back to HTML
        % \item Write the code snippets to the \ttt{.txt} file
        % \item Write the non-code textual content to the \ttt{.docx} file
    \end{enumerate}

\end{enumerate}

The lower level implementation details are mentioned in the implementations section. 

%  should i specify network stuff here?

% I don't think I should mention the evaluation section since that doesn't affect the system. That part should go into results and anyway not much to talk about over there







\chapter{Implementation Details}
% this can give a broad overview of what the prototype can and cannot do. What the input and output of the prototype look like etc.


\section{File Structure}

% ?????????????????????????

\begin{verbatim}
scraper
|-- app
|   | -- Main.hs (main driver code)
|-- test
|   |-- Spec.hs (tests)
|-- src
|   |-- Lib.hs (implementation of functions)
|-- input
|   | -- lang_train.txt (training data)
|   | -- code_train.txt (training data)
|-- output_files
|   | -- NB_code_class.txt (code snippets)
|   | -- NB_lang_class.docx (textual content)

\end{verbatim}




% in limitations can mention that need newline separated lines and we can't go beyond the granularity of a line since we did lines as documents based analysis



\section{Features and limitations}
% why mentioning this here?
The system correctly identifies the code and text portions of the web page and writes them into the \texttt{.txt} and \texttt{.docx} files as per the requirements and specifications. 

\subsubsection{Limitations}

\begin{enumerate}
    \item As mentioned, the system works on the granularity of a line and not a deeper token level granularity. Hence it can only attempt to classify whether a whole line is natural language or source code, without being able classify individual words or phrases within a line as being of a different class.
    \item Moreover, it cannot be used effectively in the case that the text source has data that is not separated by newlines, since each line is a document that needs to be classified here. However, these cases are relatively rare since most instances where code and natural language are found interspersed, there is some degree of organisation in terms of newline-separation.
    \item Since the system is relatively simplistic and unoptimized, along with the fact that it works on the Naive Bayes assumption of independence between features, it may not work well compared to other heavyweight methods in terms of accuracy. 
    \item Comments in the code written primarily in natural language will be classified as language and not code due to the nature of the classifier and the training data.
\end{enumerate}

\section{Core Implementation Details}

Some important code and data features that showcase the core implementation details of the project are shown.


% MAYBE ALSO REMOVE SOME OF THE EVALUATION STUFF CAUSE THE CODE GETTING TOO LONG    

%  NEED TO HAVE A SECTION ABOUT THE DETAILS OF THE TRAINING DATA CAUSE THAT IS OBVIOUSLY ESSENTIAL TO THE SYSTEM

\subsection{Main.hs}

\subsubsection{Training and Main Classification Pipeline}

In-line with the design, architecture and choice of tools as mentioned above, we first obtain the contents of the text source. Keeping in mind the assigned project statement that had a web page as text source, the user will supply input by giving a URL of the web page for which they wish the language and code to be separated. In principle, any text source can be used, which would just remove fetching the web page contents portion of the system. \\ 
We first just get all the textual HTML content from the web page and split the content based on newline separators. \\ Everytime the system is called, training on the data will be done since we wish to keep the Naive Bayes classifier dynamic in terms of the words in accounts for. Since training for Naive Bayes is cheap, this can be done without much additional cost.\\
\\ In the below code, we train the Naive Bayes Classifier, then predict the classes of every line of the text source and finally separate the text and the code into \ttt{.txt} and \ttt{.docx} files.

\begin{minted}[%
    breaklines,
    mathescape,
    linenos,
    numbersep=5pt,
    frame=single,
    numbersep=5pt,
    xleftmargin=0pt,
    ]{haskell}
    -- text_source is newline separated list of lines of the text_source
    let lang_train = "input/lang_train.txt"
    let code_train = "input/code_train.txt"
    mydata <- Lib.readTraining lang_train code_train
    let lang_data = fst mydata
    let code_data = snd mydata
    let trainedModel = Lib.trainNaiveBayes lang_data code_data

    let final_classes = Lib.classifyNaiveBayes text_source trainedModel
    let mapping = zip text_source final_classes
    let code_class = [x | x <- mapping, snd x == 0]
    let lang_class = [x | x <- mapping, snd x == 1]
    writeFile "output_files/NB_code_class.txt" (unlines (map fst code_class))
    writeToDocx "output_files/NB_lang_class.docx" (unlines (map fst lang_class))
   \end{minted}
   

\subsubsection{Evaluating the Classifier}

We use various test sets to evaluate the performance of the classifier on various metrics, which will be further discussed in the Results section. 
%  add various multiple kinds of test cases that have different kinds of languages and stuff

\begin{minted}[%
    breaklines,
    mathescape,
    linenos,
    numbersep=5pt,
    frame=single,
    numbersep=5pt,
    xleftmargin=0pt,
    ]{haskell}

    let testFiles = ["lang_test1.txt", "code_test1.txt","lang_test5.txt", "code_test5.txt", "lang_test6.txt", "code_test6.txt", "lang_test7.txt", "code_test7.txt", "lang_test8.txt", "code_test8.txt", "lang_test9.txt", "code_test9.txt", "lang_test10.txt", "code_test10.txt"]
    let test_evals = Lib.evalTests trainedModel testFiles

    test_results <- test_evals
   \end{minted}
   



\subsection{Lib.hs}

\subsubsection{trainNaiveBayes}

This takes the natural language and source code training data and returns the prior probability of a word being a source code word, the conditional probabilities and the vocabulary based on the training data. The description of the helper functions is given later on below. 

\begin{minted}[%
    breaklines,
    mathescape,
    linenos,
    numbersep=5pt,
    frame=single,
    numbersep=5pt,
    xleftmargin=0pt,
    ]{haskell}

-- NLA refers to the hmatrix library
trainNaiveBayes :: [String] -> [String] -> (( Double, ([Double] , [Double]) ), Vocabulary )
trainNaiveBayes natural_data source_data = 
    let source_words = concat (getWords source_data)
        natural_words = concat (getWords natural_data)
        unique_src_words = getUniqueWords source_words
        unique_natural_words = getUniqueWords natural_words
        vocab = unique_src_words ++ unique_natural_words

        -- source_data is an array of strings, where each string represents a new line in the file. So here each line here is a document here. 
        xTrain_src = myVectorizer vocab (source_data)
        xTrain_lang = myVectorizer vocab (natural_data)

        sourceCodeMatrix = xTrain_src
        naturalLanguageMatrix = xTrain_lang

        -- NLA.Matrix Double -> [Int]
        sum_src_cols = sumCols sourceCodeMatrix
        sum_lang_cols = sumCols naturalLanguageMatrix


        src_len = length source_data
        natural_len = length natural_data

        xgivenY_src = calcXGivenY src_len sum_src_cols
        xgivenY_lang = calcXGivenY natural_len sum_lang_cols

        prob_src_prior = (int2Double src_len) / (int2Double src_len + fromIntegral natural_len)

    in ((prob_src_prior, (xgivenY_src,xgivenY_lang)), vocab )
    
    
\end{minted}





\subsubsection{classifyNaiveBayes}

It takes the lines of the text source that we have to classify, along with the output of the trained Naive Bayes Model. Due to the conditional probabilities being very small numbers, we work in the logspace to prevent issues due to numerical underflow, as is the case with most Naive Bayes implementations. \\ 
\\ TODO!!!!!!!!!!!!!!!!!!!!!!!!!


\begin{minted}[%
    breaklines,
    mathescape,
    linenos,
    numbersep=5pt,
    frame=single,
    numbersep=5pt,
    xleftmargin=0pt,
    ]{haskell}
   
       
-- NLA refers to the hmatrix library
classifyNaiveBayes :: [String] -> (( Double, ([Double] , [Double]) ), Vocabulary ) -> [Int]
classifyNaiveBayes test_data trainedModel =
    let vocab = snd trainedModel
        xTest = (NLA.toLists (myVectorizer vocab (test_data)))
        test_len = length xTest
        y = NLA.fromLists [(replicate (test_len) (int2Double 0))]

        -- [Double]
        xgivenY_src = fst (snd (fst trainedModel))
        xgivenY_lang = snd (snd (fst trainedModel))
        prob_src_prior = fst (fst trainedModel)

        log_src = map log xgivenY_src
        log_lang = map log xgivenY_lang

        log_matrix = NLA.tr (NLA.fromLists [log_src, log_lang])

        -- [Double,Double]
        -- print (map typeOf (head (DM.toLists log_matrix)))

        -- mult
        prob1 = (NLA.fromLists xTest) NLA.<> (log_matrix)

        -- [Double,Double]
        -- print (map typeOf (head (DM.toLists prob1)))
        logp = log prob_src_prior
        log_not_p = log (1 - prob_src_prior)

        prob1_trans = NLA.toLists (NLA.tr prob1)

        prob2 = map (\x -> x + logp) (head prob1_trans)
        prob3 = map (\x -> x + log_not_p) (head (tail prob1_trans))

        combined = [prob2, prob3]
        combined_mat = NLA.tr (NLA.fromLists combined)
        
        final_probs = map (\x -> if (head x) > (head (tail x)) then 0 else 1) (NLA.toLists combined_mat)
    in final_probs       
\end{minted}


\subsubsection{Important Helpers}
\begin{enumerate}
    \item \ttt{myVectorizer : } gives a matrix where each row corresponds to a document i.e. line and each column is a word in the vocabulary. So an entry $(i,j)$ represents the word count of word $j$ from the vocab in document $i$. If supplied empty lists then we return a dummy value.
    \item \ttt{matrixRow : } It corresponds to one row in the above matrix. For each word in the vocabulary it gets the count of that word in the current document.
    \item \ttt{wordCounts : } For a document, creates a mapping between each word in the document and the count of that word in the document.
    \item \ttt{sumCols : } Finds the sum of every column in the matrix i.e. finds the total occurences of each word across all documents.
    \item \ttt{calcXGivenY : } For each word in the vocabulary, we sum the number of times it has appeared in source code documents. We divide by the total number of source code documents to get the probability of each word being in source code. We do a similar thing for natural language. We add small decimal numbers as a form of Laplace smoothing to avoid 0 probability in case of completely unseen words for the unseen data.
\end{enumerate}

\begin{minted}[%
    breaklines,
    mathescape,
    linenos,
    numbersep=5pt,
    frame=single,
    numbersep=5pt,
    xleftmargin=0pt,
    ]{haskell}
    myVectorizer :: Vocabulary -> [Document] -> NLA.Matrix Double
    myVectorizer vocab docs
        | vocab == [] = NLA.fromLists [[int2Double 0]] -- sinceFromlists doesn't accept empty lists
        | docs == [] = NLA.fromLists [[int2Double 0]]
        | otherwise = NLA.fromLists [matrixRow vocab doc | doc <- docs]

    matrixRow :: Vocabulary -> Document -> [Double]
    matrixRow vocab doc = [fromMaybe (int2Double 0) (lookup vocab_word mywordcounts) | vocab_word <- vocab]
    where mywordcounts = wordCounts doc

    wordCounts :: Document -> [(String, Double)]
    wordCounts doc = Data.Map.toList $ fromListWith (+) [(oneword, int2Double 1) | oneword <- words doc]

    sumCols :: NLA.Matrix Double -> [Double]
    sumCols matrix = map sum (NLA.toLists (NLA.tr matrix))

    calcXGivenY :: Int -> [Double] -> [Double]
    calcXGivenY mylen my_cols_sum =  map (\x -> x + 0.001 / int2Double (mylen) + 0.9 ) my_cols_sum

   \end{minted}




\subsubsection{Writers to files}
\texttt{writeToDocx} takes the text that has been classified as natural language and writes it into a \texttt{.docx} file after some formatting in order to massage the text into a Pandoc-friendly format due to the absence of functionality to read a plainText document or string.

\begin{minted}[%
 breaklines,
 mathescape,
 linenos,
 numbersep=5pt,
 frame=single,
 numbersep=5pt,
 xleftmargin=0pt,
 ]{haskell}
 writeToDocx :: String -> String -> IO ()
 writeToDocx filepath lang_class  = do
     pandoc_lang <- runIO $ readHtml def ( TextConv.convertText ("<p>" ++ lang_class ++ "</p>" :: String ) :: T.Text )
 
     case pandoc_lang of
         Right x -> do
             y <- runIO $ writeDocx def x
             case y of
                 Right direct_pan -> do
                     LBS.writeFile filepath direct_pan
 
                 Left err -> Prelude.putStrLn $ "Error with pandoc writeDocx: " ++ show err
 
         Left err -> Prelude.putStrLn $ "Error parsing pandoc for natural language " ++ show err
 
     putStrLn "Completed writing to docx"
\end{minted}


\subsubsection{Evaluating Functions}

\begin{minted}[%
    breaklines,
    mathescape,
    linenos,
    numbersep=5pt,
    frame=single,
    numbersep=5pt,
    xleftmargin=0pt,
    ]{haskell}

    evalTests :: (( Double, ([Double] , [Double]) ), Vocabulary ) -> [String] -> IO [(Double, Double, Double, Double)]
    evalTests _ [] = return []
    evalTests trainedModel (x:y:xs) = do
        lang_test <- readFile ("cases/" ++ x)
        src_test <- readFile ("cases/" ++ y)
        let test_data = (lines src_test) ++ (lines lang_test)
        let final_probs = Lib.classifyNaiveBayes test_data trainedModel
        let mapping = zip test_data final_probs
        let src_test_len = length (lines src_test)
        let lang_test_len = length (lines lang_test)
        let yTest = replicate src_test_len 0 ++ replicate lang_test_len 1
        let test_accuracy_mapping = zip yTest final_probs
        -- precision_code, recall_code, precision_lang, recall_lang
        rest <- evalTests trainedModel xs
        return $ (Lib.evaluateNaiveBayes test_accuracy_mapping):rest

    evaluateNaiveBayes :: [(Int, Int)] -> (Double, Double, Double, Double)
    evaluateNaiveBayes mydata = 
        let total_actual_lang = sum (map fst mydata)
            total_actual_code = (length mydata) - total_actual_lang
            code_correct = filter (== (0,0)) mydata
            lang_correct = filter (== (1,1)) mydata
            code_wrong  = filter (== (0,1)) mydata
            lang_wrong  = filter (== (1,0)) mydata
    
            num_code_correct = length code_correct
            num_lang_correct = length lang_correct
            num_code_wrong = length code_wrong
            num_lang_wrong = length lang_wrong
    
            -- need to keep in mind that paper has used different metrics
            
    
            precision_code = int2Double num_code_correct / (int2Double num_code_correct + int2Double num_lang_wrong)
    
            recall_code = int2Double num_code_correct / ((int2Double num_code_correct) + int2Double num_code_wrong)
    
            precision_lang = int2Double num_lang_correct / (int2Double num_lang_correct + int2Double num_code_wrong)
    
            recall_lang = int2Double num_lang_correct / ((int2Double num_lang_correct) + int2Double num_lang_wrong)
    
        in (precision_code, recall_code, precision_lang, recall_lang)
    
        
   \end{minted}

\newpage 
\section{Training Data}

Due to paucity of appropriate training data for this project with respect to scope and relevance, I curated my own small training dataset comprising of code and natural language that is typically seen accompanying the code. While there were datasets with source code and natural language, they were of a larger scale than I intended, and I also wanted to train the model on a smaller dataset to see how lightweight it can be \\ 
\\ I made sure to include a variety of programming languages in the source_code dataset to ensure wider generalisability. The statistics of the training data are given as follows, along with the sources where I took them from. The code was cleaned to remove comments and other non-code text.

%  unskew this away from C

%  can put a table here which summarises the training data

% insert table here that has as headings the name of a programming language and the lines of code in the training dataset
\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Programming Language} & \textbf{Lines of Code} & Source \\
        \hline
        C & 1171 & \href{https://github.com/karpathy/llm.c/blob/master/train_gpt2.c}{Andrej Karpathy's Github} \\
        \hline
        Python & 461 & \href{https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/covariance/_robust_covariance.py}{Sklearn's Github} \\
        \hline
        Java & 201 & \href{https://github.com/jenkinsci/jenkins/tree/master}{Jenkins Github} \\
        \hline
    \end{tabular}
    \caption{Training Data Statistics (Code)}
    \label{tab:training-data}
\end{table}





\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Description} & \textbf{Words and Lines} & Source \\
        \hline
            
        CS50 Lec1 & 780 and 37 & \href{https://cs50.harvard.edu/college/2023/spring/notes/1/}{CS50 Lecture 1} \\
        \hline
        CS50 Lec6 & 229 and 11 & \href{https://cs50.harvard.edu/x/2023/notes/6/}{CS50 Lecture 6} \\
        \hline
        Python 4 Everybody Text & 4910 and 589 & \href{https://www.py4e.com/code3/intro.txt}{Python 4 Everybody} \\
        \hline

    \end{tabular}
    \caption{Training Data Statistics (Language)}
    \label{tab:training-data}
\end{table}





\section{Challenges and Mitigations}

Initially, I started the project by separating the code and text based on their HTML tags for the midterm project evaluation, but after feedback from Professor I decided to make the project much more generalized to any given text source instead of just HTML content due to the fragile nature of a system based on the latter. \\ I decided to implement the Naive Bayes algorithm from scratch due to it's ease of interpretability, simplicity, lightweightedness and surprisingly strong results. A brief outline of some challenges and mitigations are as follows:
\begin{enumerate}
    \item Before deciding on implementing Naive Bayes from scratch, I thought about using various methods like Hidden Markov Models and Support Vector Machines to create an ensemble learning method to classify text and code. However, library support in Haskell for these was not of good quality, hence I decided to implement Naive Bayes on my own.
    %  mention how good performance?
    \item In the implementation of Naive Bayes, another hurdle I faced was the inefficiency of the default \ttt{matrix} library in doing calculations with extremely large and sparse matrices, which was restricting me to training and testing sets of very small sizes, albeit with considerably good performance. So I looked for more efficient libraries and found the \ttt{hmatrix} library, which sped up computations by a large extent. Earlier a training and classification that took 2.5 minutes overall, now happened in 2.5 seconds. 
    \item The paucity of appropriate training data was also an issue. The few datasets that were relevant to the project, such as those containing source code along with some form of natural language (usually in the form of code comments) were all restricted to one language and were also immense for a classifier implemented from scratch without optimisations. Hence, I curated my own modestly size source code and natural language training data based on manual searching online. The small size of the dataset also highlighted how the lightweight model was still performing relatively well. 
    \item The Haskell language was itself relatively challenging to get used to throughout the project due to it's difference from the imperative languages I am used to. But after learning lambda calculus and spending time with the language, it became easier to use and I could appreciate the elegance of functional programming more, even though I am still not as accustommed to this paradigm as I would like to be. 
\end{enumerate}




% maybe this can go after implementation details
\chapter{Tooling and Testing}

\section {Tools and Languages}
% tech stack
% need to be clear on cabal stack and haskell and stuff


\subsection{Languages}

Only Haskell was used for the project as mentioned in the problem statement

\subsection{Tools}

\begin{enumerate}
    \item The Glasgow Haskell Compiler (GHC) is used for compilation.
    \item \href{https://docs.haskellstack.org/en/stable/}{Stack} is used as the build tool. This manages installing project dependencies, building and running the project and testing the project.
    
    % say that hmatrix was used cause fastest
    \item \textbf{Notable Libraries}:
    \begin{enumerate}
        \item \textbf{hmatrix : } The fastest numerical linear algebra library that I could find in order to speed up the entire process, according to one benchmark. 
        %  CITE THIS!!!!!!!
        \item \textbf{hUnit : } The basic Haskell unit testing library
    \end{enumerate} 
\end{enumerate}






\section{Testing}
% test individual functions 
% for the midterm eval at the end if have time (need to) then can write one big test 

% \section{Tools to be used for Testing}



\subsection{Test Suite Outline}

\begin{enumerate}
    \item \textbf{Unit Testing : } The following functions were unit tested in the \ttt{Spec.hs} file
    \begin{enumerate}
        \item \ttt{myVectorizer : }
        \item \ttt{matrixRow : }
        \item \ttt{wordCounts : }
        \item \ttt{getWords : }
        \item \texttt{getHTML : }
        \item \ttt{getUniqueWords}
        \item \ttt{getHTML}
        \begin{enumerate}
            \item Test if the function returns the correct HTML for various URLs by using sample inputs and outputs.
            \item Test if the function gracefully handles invalid URLs with error handling.
            \item Test if the function handles network errors and HTTPS errors with error handling.
        \end{enumerate}

        \item \texttt{writeToDocx : }
        While File I/O cannot be unit-tested in the traditional sense, we can still manually test the file outputs.
        \begin{enumerate}
            \item Test if the function correctly writes the content of the input Tag String to the \texttt{.docx} file using sample input output
        \end{enumerate}

    \end{enumerate}

    \item \textbf{Performance Testing : } 
    % criterion and command line tools?
    \begin{enumerate}
        \item Test the time taken for the full system to execute from start to finish
        \item Test the resource consumption of the system
    \end{enumerate}
    \item \textbf{Functional Testing : } \\ Test whether the program meets the requirements by using sample input URLs and sample output files
\end{enumerate}


\subsection{Test Report}

\begin{enumerate}
    \item \textbf{Unit Testing : }
    
\end{enumerate}






\chapter{Plan for Completion}
% will test. will do error handling. formal verification and validation stuff and maybe make more robust for other urls also idk. Getting better performance


\begin{itemize}
    \item Implement error handling gracefully wherever possible so that no unhandled errors can occur.
    \item Implement the test suite
    \item Attempt to make the scraper more generalized so that it can work with other websites and other HTML structures.
    \item Document the code well
    \item Explore methods of separating the code from the non-code textual content that are independent of HTML structure. Such methods were mentioned in the analysis and implementation of these methods will make the scraper more generalizable. 
\end{itemize}


\chapter{Results and Discussions}

The classifier was run on various natural language and source code files and the results were collected. Metrics like precison and recall with respect to code and language, along with the F-measure were calculated. We found that even with the highlight simplistic design and the strong Naive Bayes assumptions, the classifier could perform surprisingly well in classifying lines as code or natural language, subject to the conditions that the code was present in the training data.


% insert table here

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Case} & Code Details \\
        \hline
        Test 1 & \href{https://eli.thegreenplace.net/2018/type-erasure-and-reification/}{Supplied Web Page} containing C, Python, Java \\
        \hline
        Test 5 & Assembly Code from the \href{https://github.com/chrislgarry/Apollo-11/blob/master/Luminary099/AOSTASK_AND_AOSJOB.agc}{Apollo Guidance Computer} \\
        \hline
        Test 6 & SQL Code from \href{https://dev.to/tyzia/example-of-complex-sql-query-to-get-as-much-data-as-possible-from-database-9he}{this blog} \\
        \hline
        Test 7 & Haskell Code from \href{https://github.com/koalaman/shellcheck}{ShellCheck}\\
        \hline
        Test 8 & Mix of Python and C Code\\
        \hline
        Test 9 & Java Code\\
        \hline
        Test 10 & Python Code\\
        \hline
    \end{tabular}
    \caption{Test Case Details (Code)}
    \label{tab:results}
\end{table}


Details about the natural language test cases are not provided since difference in the data sources for natural language was found to not have much difference in accuracy.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Test} & \textbf{Precision (Code)} & \textbf{Recall (Code)} & \textbf{Precision (Language)} & \textbf{Recall (Language)} \\
        \hline
        1 & 0.93 & 0.98 & 0.96 & 0.86 \\
        \hline
        5 & 0.97 & 1.0 & 1.0 & 0.47 \\
        \hline
        6 & 0.87 & 0.92 & 0.67 & 0.53 \\
        \hline
        7 & 0.93 & 0.93 & 0.47 & 0.46 \\
        \hline
        8 & 0.95 & 0.89 & 0.38 & 0.58 \\
        \hline
        9 & 0.89 & 0.99 & 0.89 & 0.5 \\
        \hline
        10 & 0.97 & 0.9 & 0.43 & 0.71 \\
        \hline
    \end{tabular}
    \caption{Test Case Results}
    \label{tab:results}
\end{table}





\begin{table}[h]
        \centering
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            \textbf{Test} & \textbf{Precision (Code)} & \textbf{Recall (Code)} & \textbf{Precision (Language)} & \textbf{Recall (Language)} \\
            \hline
            1 & 0.95 & 0.95 & 0.95 & 0.95 \\
            \hline
            5 & 0.75 & 0.75 & 0.75 & 0.75 \\
            6 & 0.90 & 0.90 & 0.90 & 0.90 \\
            \hline
            7 & 0.85 & 0.85 & 0.85 & 0.85 \\
            \hline
            8 & 0.80 & 0.80 & 0.80 & 0.80 \\
            \hline
            9 & 0.80 & 0.80 & 0.80 & 0.80 \\
            \hline
            9 & 0.80 & 0.80 & 0.80 & 0.80 \\
            \hline
        \end{tabular}
        \caption{Results of the Classifier}
        \label{tab:results}
\end{table}

%  in the results can have one example of some natural language or code that gets misclassified

%  graphs and stuff


%  pie charts and stuff


% tables and stuff 


\chapter{Conclusions}
%  should highlight contributions
%  lightweight text and code separation in functional programming language so it can be easily verified or smth
% it's also very interpretable and linear time complexity stuff



\chapter{Extensions and Future Work}



\begin{thebibliography}{9}

    \bibitem{bachemail}
    A. Bacchelli, M. D’Ambros, and M. Lanza, “Extracting source code from e-mails,” in Program Comprehension (ICPC), 2010 IEEE 18th International Conference on, June 2010, pp. 24–33.

    \bibitem{bachcontent}
    A. Bacchelli, T. Dal Sasso, M. D'Ambros and M. Lanza, "Content classification of development emails," 2012 34th International Conference on Software Engineering (ICSE), Zurich, Switzerland, 2012, pp. 375-385, doi: 10.1109/ICSE.2012.6227177

    \bibitem{chattter}
    Chatterjee, Preetha \& Gause, Benjamin \& Hedinger, Hunter \& Pollock, Lori. (2017). Extracting Code Segments and Their Descriptions from Research Articles. 10.1109/MSR.2017.10. 

    \bibitem{cerulohmm}
    L. Cerulo, M. Ceccarelli, M. Di Penta, and G. Canfora, “A hidden markov model to detect coded information islands in free text,” in Source Code Analysis and Manipulation (SCAM), 2013 IEEE 13th International Working Conference on, Sept 2013, pp. 157–166.

\end{thebibliography}













\end{document}